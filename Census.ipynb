{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "from fairtree import * \n",
    "from gmm_fairlet_decomposition import VanillaFairletDecomposition, MCFFairletDecomposition\n",
    "from scipy.spatial import distance\n",
    "from MWD_utils import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('Data/income/adult.data',sep=',',header=None)\n",
    "df[9].replace({\" Male\": 1, \" Female\": 0}, inplace=True)\n",
    "A = df[9].to_numpy()\n",
    "df = df[[9,0,2,4,10,12]]\n",
    "data = df.to_numpy()\n",
    "print('Dataset fraction',sum(A)/len(A))\n",
    "np.random.seed(0) \n",
    "n_class0 = 170\n",
    "n_class1 = 330\n",
    "selection = np.random.choice(np.where(data[:,0]==0)[0],size=n_class0,replace=False)\n",
    "data0 = data[selection]\n",
    "selection = np.random.choice(np.where(data[:,0]==1)[0],size=n_class1,replace=False)\n",
    "data1 = data[selection]\n",
    "data = np.concatenate((data0,data1))\n",
    "np.random.shuffle(data)\n",
    "A = data[:,0] \n",
    "data = data[:,1:] \n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data= scaler.fit_transform(data) \n",
    "print(data.shape)\n",
    "reds = np.where(A==0)[0]\n",
    "blues = np.where(A==1)[0]\n",
    "print('fraction in subsample',sum(A)/len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmedian decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmedian decomposition construction does not depend on a changing metric space for the different number of \n",
    "# mixture components, so we can generate it outside the experiment loop. \n",
    "\n",
    "# Generating (1,2)-fairlet decomposition\n",
    "p = 1 \n",
    "q = 2 \n",
    "\n",
    "print(\"Constructing tree...\")\n",
    "fairlet_s = time.time()\n",
    "root = build_quadtree(data)\n",
    "\n",
    "print(\"Doing fair clustering...\")\n",
    "_, kmedian_fairlet_centers, kmedian_fairlets = tree_fairlet_decomposition(p, q, root, data, A)\n",
    "fairlet_e = time.time()\n",
    "\n",
    "print(f'N fairlets {len(kmedian_fairlet_centers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control variables for experiments \n",
    "# Runtime is approximately 1 minute pr iteration. I.e. total_runtime ≈ 1min * iterations * (n_components-1)\n",
    "\n",
    "n_components = 20      # The largest number of components. Will run experiments from 2 to n_components\n",
    "iterations   = 5      # Number of iterations of different random seeds for each number of mixture components\n",
    "cov_type     = 'full' # Covariance structure of the GMM. Options: {‘full’, ‘tied’, ‘diag’}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing arrays to save results\n",
    "# Full is the traditional / colorblind GMM model on the full original dataset.\n",
    "# The traditional model does not use a decomposition, so we do not need to initialize it.\n",
    "# mcf is the GMM MWD minimum cost flow decomposition. \n",
    "Cost_full    = np.zeros([19,2])\n",
    "Cost_mcf     = np.zeros([19,2])\n",
    "Cost_kmedian = np.zeros([19,2])\n",
    "\n",
    "Decomp_mcf     = np.zeros([19,2])\n",
    "Decomp_kmedian = np.zeros([19,2])\n",
    "\n",
    "Balance_full    = np.zeros([19,2])\n",
    "Balance_mcf     = np.zeros([19,2])\n",
    "Balance_kmedian = np.zeros([19,2])\n",
    "\n",
    "Entropy_full    = np.zeros([19,2])\n",
    "Entropy_mcf     = np.zeros([19,2])\n",
    "Entropy_kmedian = np.zeros([19,2])\n",
    "\n",
    "Like_mcf     = np.zeros([19,2])\n",
    "Like_kmedian = np.zeros([19,2])\n",
    "\n",
    "\n",
    "k = data.shape[1] # multivariate dimension of the data.\n",
    "for components in range(2,n_components+1):\n",
    "\n",
    "    # Running lists for each number of mixture components for the different random seeds \n",
    "    # Balance (B), total cost (C), entropy (H), decomposition cost (D), and decomposition likelihood (L)\n",
    "    \n",
    "    B_full = []\n",
    "    C_full = []\n",
    "    H_full = []\n",
    "\n",
    "\n",
    "    D_mcf = []\n",
    "    B_mcf = []\n",
    "    C_mcf= []\n",
    "    H_mcf = []\n",
    "\n",
    "    D_kmedian = []\n",
    "    B_kmedian = []\n",
    "    C_kmedian= []\n",
    "    H_kmedian = []\n",
    "    \n",
    "    L_mcf = []\n",
    "    L_kmedian = []\n",
    "\n",
    "    print('\\nComponents:',components)\n",
    "\n",
    "    for random_state in range(iterations):\n",
    "        print('\\nIteration',random_state+1, 'of',iterations)\n",
    "    \n",
    "        # ======== Colorblind GMM ========\n",
    "        gm_trad = GaussianMixture(n_components=components, random_state=random_state,covariance_type=cov_type,max_iter=100,n_init=3,init_params='k-means++').fit(data)\n",
    "        gamma_metric = gm_trad.predict_proba(data)\n",
    "\n",
    "        # Saving colorblind distribution parameters to instantiate G metric and compute model-weighted distances\n",
    "        metric_weights = gm_trad.weights_\n",
    "        metric_means = gm_trad.means_\n",
    "        metric_covariances = gm_trad.covariances_\n",
    "\n",
    "        cost_full = MHD_GMM_Cost(gamma_metric,data,metric_weights,metric_means,metric_covariances) \n",
    "        print('Traditional Cost',cost_full)        \n",
    "\n",
    "        # Fairness for the colorblind model \n",
    "        H1,H_ratio1,balance1,p1 = soft_fairness(gamma_metric,A)   # for category 1 \n",
    "        H2,H_ratio2,balance2,p2 = soft_fairness(gamma_metric,1-A) # for category 2\n",
    "        balance = np.min([balance1,balance2])\n",
    "        H_ratio = np.min([H_ratio1,H_ratio2])\n",
    "     \n",
    "        C_full.append(cost_full)\n",
    "        B_full.append(balance)\n",
    "        H_full.append(H_ratio)\n",
    "        print('Full Balance',balance)\n",
    "      \n",
    "    \n",
    "        # ======== GMM MWD MCF =========\n",
    "        print('GMM MCF decomposition...')\n",
    "        # Instantiating the MCF Decomposition. Running (1,2)-fairlet decomposition\n",
    "        mcf = MCFFairletDecomposition(list(blues), list(reds), 2, None, list(data),metric_weights,metric_means,metric_covariances)\n",
    "\n",
    "        # Computing the distance matrix between blue and red nodes\n",
    "        mcf.compute_distances()\n",
    "\n",
    "        # Adding nodes and edges\n",
    "        mcf.build_graph(plot_graph=False)\n",
    "\n",
    "        # Decomposing for fairlets \n",
    "        mcf_fairlets, mcf_fairlet_centers, mcf_fairlet_costs = mcf.decompose()\n",
    "    \n",
    "        # Finding the decomposition cost\n",
    "        decomp_cost = []\n",
    "        for f in mcf_fairlets:\n",
    "            decomp_cost.append(center_cost(data[f],metric_weights,metric_means,metric_covariances))   \n",
    "        decomp_cost = sum(decomp_cost)\n",
    "        print('MCF decomposition cost:',decomp_cost)\n",
    "\n",
    "        # Identifying fairlet centers  \n",
    "        mcf_centers_mean = []\n",
    "        for i in range(len(mcf_fairlets)):\n",
    "            mcf_centers_mean.append(np.mean(data[mcf_fairlets[i]],axis=0))\n",
    "        mcf_centers_mean = np.array(mcf_centers_mean)\n",
    "        \n",
    "        #Likelihood of decomposition\n",
    "        mcf_fairlet_likelihood = []\n",
    "        for i in range(len(mcf_fairlets)):\n",
    "            for j in range(len(data[mcf_fairlets[i]])):  \n",
    "                l = mcf_fairlets[i][j] # index of data point x\n",
    "                prob_at_x = gm_trad.predict_proba(data[l:l+2])[0] # the component probability at x. l+2 to make 'predict_proba' work. Only saing the first entry [0]\n",
    "                dist,G = MWDG(data[mcf_fairlet_centers[i]],data[mcf_fairlets[i]][j],metric_weights,metric_means,metric_covariances,prob_at_x)                \n",
    "                mcf_fairlet_likelihood.append(logL(k,G,dist,G_metric=True)) \n",
    "                  \n",
    "                    \n",
    "        decomp_likelihood = np.sum(mcf_fairlet_likelihood)  # likelihood cost\n",
    "       \n",
    "\n",
    "        # Algorithm 1 on GMM MWD decomposition \n",
    "        fairlet_sizes = [len(mcf_fairlets[i]) for i in range(len(mcf_fairlet_centers))] # size of fairlets\n",
    "        Dbar=[]\n",
    "        for i in range(len(mcf_fairlet_centers)):\n",
    "            for j in range(fairlet_sizes[i]):\n",
    "                Dbar.append(mcf_centers_mean[i])\n",
    "        Dbar = np.asarray(Dbar)\n",
    "        \n",
    "        # Computing mixture model on Dbar\n",
    "        gm = GaussianMixture(n_components=components, random_state=random_state,covariance_type=cov_type,max_iter=100,n_init=3,init_params='k-means++').fit(Dbar)\n",
    "        gamma = gm.predict_proba(Dbar)\n",
    "        weights = gm.weights_\n",
    "        means = gm.means_\n",
    "        covariances = gm.covariances_\n",
    "\n",
    "        Dbar_cost = MHD_GMM_Cost(gamma,Dbar,weights,means,covariances)\n",
    "        total_cost = decomp_cost + Dbar_cost\n",
    "        \n",
    "        print('MCF Total cost:',total_cost)        \n",
    "                \n",
    "        # Assigning center responsibilities to fairlet members with the right index \n",
    "        center_gamma = gm.predict_proba(mcf_centers_mean) # responsibility of centers\n",
    "        gamma=np.zeros([data.shape[0],components])\n",
    "        for i in range(len(mcf_centers_mean)):\n",
    "                gamma[mcf_fairlets[i]] = center_gamma[i]\n",
    "    \n",
    "        # Fairness\n",
    "        H1,H_ratio1,balance1,p1 = soft_fairness(gamma,A)   # For category 1 \n",
    "        H2,H_ratio2,balance2,p2 = soft_fairness(gamma,1-A) # For category 2\n",
    "        balance = np.min([balance1,balance2])\n",
    "        H_ratio = np.min([H_ratio1,H_ratio2])\n",
    "    \n",
    "        D_mcf.append(decomp_cost)\n",
    "        C_mcf.append(total_cost)\n",
    "        B_mcf.append(balance)\n",
    "        H_mcf.append(H_ratio)\n",
    "        L_mcf.append(decomp_likelihood)\n",
    "    \n",
    "\n",
    "        # ======== Euclidean Kmedian Decomp ======== \n",
    "        \n",
    "        # Finding GMM cost of kmedian decomposition \n",
    "        kmedian_fairlet_costs = []\n",
    "        for i in range(len(kmedian_fairlets)):\n",
    "            for j in range(len(data[kmedian_fairlets[i]])):\n",
    "                kmedian_fairlet_costs.append(MWDistance(data[kmedian_fairlet_centers[i]],data[kmedian_fairlets[i]][j],metric_weights,metric_means,metric_covariances))  \n",
    "        decomp_cost = sum(kmedian_fairlet_costs)\n",
    "        \n",
    "        #Likelihood of decomposition\n",
    "        kmedian_fairlet_likelihood = []\n",
    "        for i in range(len(kmedian_fairlets)):\n",
    "            for j in range(len(data[kmedian_fairlets[i]])):  \n",
    "                l = kmedian_fairlets[i][j] # index of data point x\n",
    "                prob_at_x = gm_trad.predict_proba(data[l:l+2])[0] # the component probability at x. l+2 to make 'predict_proba' work. Only saing the first entry [0]\n",
    "                dist,G = MWDG(data[kmedian_fairlet_centers[i]],data[kmedian_fairlets[i]][j],metric_weights,metric_means,metric_covariances,prob_at_x)    \n",
    "                kmedian_fairlet_likelihood.append(logL(k,G,dist,G_metric=True))            \n",
    "\n",
    "        decomp_likelihood = np.sum(kmedian_fairlet_likelihood)               \n",
    "\n",
    "\n",
    "        # Algorithm 1 on kmedian decomposition\n",
    "        fairlet_sizes = [len(kmedian_fairlets[i]) for i in range(len(kmedian_fairlet_centers))] # size of fairlets\n",
    "        Dbar=[]\n",
    "        for i in range(len(kmedian_fairlet_centers)):\n",
    "            for j in range(fairlet_sizes[i]):\n",
    "                Dbar.append(data[kmedian_fairlet_centers[i]])\n",
    "        Dbar = np.asarray(Dbar)\n",
    "         \n",
    "        # Computing mixture model on decomposition\n",
    "        gm = GaussianMixture(n_components=components, random_state=random_state,covariance_type=cov_type,max_iter=100,n_init=3,init_params='k-means++').fit(Dbar)\n",
    "        gamma = gm.predict_proba(Dbar)\n",
    "        weights = gm.weights_\n",
    "        means = gm.means_\n",
    "        covariances = gm.covariances_\n",
    "\n",
    "        Dbar_cost = MHD_GMM_Cost(gamma,Dbar,weights,means,covariances) \n",
    "        total_cost = decomp_cost + Dbar_cost\n",
    "\n",
    "        print('Kmedian Total cost:',total_cost)        \n",
    "        \n",
    "        # Assigning center responsibilities to fairlet members with correct index\n",
    "        center_gamma = gm.predict_proba(data[kmedian_fairlet_centers]) # responsibility of centers\n",
    "        gamma=np.zeros([data.shape[0],components])\n",
    "        for i in range(len(kmedian_fairlet_centers)):\n",
    "                gamma[kmedian_fairlets[i]] = center_gamma[i]\n",
    "    \n",
    "        # Fairness\n",
    "        H1,H_ratio1,balance1,p1 = soft_fairness(gamma,A)   # for category 1 \n",
    "        H2,H_ratio2,balance2,p2 = soft_fairness(gamma,1-A) # for category 2\n",
    "        balance = np.min([balance1,balance2])\n",
    "        H_ratio = np.min([H_ratio1,H_ratio2])\n",
    "        \n",
    "        D_kmedian.append(decomp_cost)\n",
    "        C_kmedian.append(total_cost)\n",
    "        B_kmedian.append(balance)\n",
    "        H_kmedian.append(H_ratio)\n",
    "        L_kmedian.append(decomp_likelihood)        \n",
    "    \n",
    "    # Generating mean and standard deviation values for the current number of mixture components\n",
    "    cost_mean_full    = np.mean(C_full)\n",
    "    cost_std_full     = np.std(C_full)\n",
    "    balance_mean_full = np.mean(B_full)\n",
    "    balance_std_full  = np.std(B_full)\n",
    "    entropy_mean_full = np.mean(H_full)\n",
    "    entropy_std_full  = np.std(H_full)\n",
    "\n",
    "\n",
    "    decomp_mean_mcf    = np.mean(D_mcf)\n",
    "    decomp_std_mcf     = np.std(D_mcf)\n",
    "    cost_mean_mcf    = np.mean(C_mcf)\n",
    "    cost_std_mcf     = np.std(C_mcf)\n",
    "    balance_mean_mcf = np.mean(B_mcf)\n",
    "    balance_std_mcf  = np.std(B_mcf)\n",
    "    entropy_mean_mcf = np.mean(H_mcf)\n",
    "    entropy_std_mcf  = np.std(H_mcf)\n",
    "    like_mean_mcf = np.mean(L_mcf)\n",
    "    like_std_mcf  = np.std(L_mcf)\n",
    "    \n",
    "    decomp_mean_kmedian    = np.mean(D_kmedian)\n",
    "    decomp_std_kmedian     = np.std(D_kmedian)    \n",
    "    cost_mean_kmedian    = np.mean(C_kmedian)\n",
    "    cost_std_kmedian     = np.std(C_kmedian)\n",
    "    balance_mean_kmedian = np.mean(B_kmedian)\n",
    "    balance_std_kmedian  = np.std(B_kmedian)\n",
    "    entropy_mean_kmedian = np.mean(H_kmedian)\n",
    "    entropy_std_kmedian  = np.std(H_kmedian)\n",
    "    like_mean_kmedian = np.mean(L_kmedian)\n",
    "    like_std_kmedian  = np.std(L_kmedian)\n",
    "    \n",
    "    # Saving mean and standard deviation values for the current number of components\n",
    "    Cost_full[components-2,0]        = cost_mean_full\n",
    "    Cost_mcf[components-2,0]         = cost_mean_mcf\n",
    "    Cost_kmedian[components-2,0]     = cost_mean_kmedian\n",
    "    \n",
    "    Decomp_mcf[components-2,0]         = decomp_mean_mcf\n",
    "    Decomp_kmedian[components-2,0]     = decomp_mean_kmedian\n",
    "\n",
    "    Balance_full[components-2,0]     = balance_mean_full \n",
    "    Balance_mcf[components-2,0]      = balance_mean_mcf\n",
    "    Balance_kmedian[components-2,0]  = balance_mean_kmedian\n",
    "\n",
    "    Entropy_full[components-2,0]     = entropy_mean_full \n",
    "    Entropy_mcf[components-2,0]      = entropy_mean_mcf\n",
    "    Entropy_kmedian[components-2,0]  = entropy_mean_kmedian\n",
    "\n",
    "    Cost_full[components-2,1]        = cost_std_full\n",
    "    Cost_mcf[components-2,1]         = cost_std_mcf\n",
    "    Cost_kmedian[components-2,1]     = cost_std_kmedian\n",
    "    \n",
    "    Decomp_mcf[components-2,1]         = decomp_std_mcf\n",
    "    Decomp_kmedian[components-2,1]     = decomp_std_kmedian\n",
    "\n",
    "    Balance_full[components-2,1]     = balance_std_full \n",
    "    Balance_mcf[components-2,1]      = balance_std_mcf\n",
    "    Balance_kmedian[components-2,1]  = balance_std_kmedian\n",
    "\n",
    "    Entropy_full[components-2,1]     = entropy_std_full \n",
    "    Entropy_mcf[components-2,1]      = entropy_std_mcf\n",
    "    Entropy_kmedian[components-2,1]  = entropy_std_kmedian\n",
    "    \n",
    "    \n",
    "    Like_mcf[components-2,1]      = like_std_mcf\n",
    "    Like_kmedian[components-2,1]  = like_std_kmedian\n",
    "    Like_mcf[components-2,0]      = like_mean_mcf\n",
    "    Like_kmedian[components-2,0]  = like_mean_kmedian\n",
    "    \n",
    "print('\\nDone!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('results/census/Cost_full.pkl', 'wb')\n",
    "pickle.dump(Cost_full, output)\n",
    "output.close()\n",
    "output = open('results/census/Cost_vanilla.pkl', 'wb')\n",
    "pickle.dump(Cost_vanilla, output)\n",
    "output.close()\n",
    "output = open('results/census/Cost_mcf.pkl', 'wb')\n",
    "pickle.dump(Cost_mcf, output)\n",
    "output.close()\n",
    "output = open('results/census/Cost_kmedian.pkl', 'wb')\n",
    "pickle.dump(Cost_kmedian, output)\n",
    "output.close()\n",
    "\n",
    "\n",
    "output = open('results/census/Decomp_mcf.pkl', 'wb')\n",
    "pickle.dump(Decomp_mcf, output)\n",
    "output.close()\n",
    "output = open('results/census/Decomp_kmedian.pkl', 'wb')\n",
    "pickle.dump(Decomp_kmedian, output)\n",
    "output.close()\n",
    "\n",
    "output = open('results/census/Balance_full.pkl', 'wb')\n",
    "pickle.dump(Balance_full, output)\n",
    "output.close()\n",
    "output = open('results/census/Balance_mcf.pkl', 'wb')\n",
    "pickle.dump(Balance_mcf, output)\n",
    "output.close()\n",
    "output = open('results/census/Balance_kmedian.pkl', 'wb')\n",
    "pickle.dump(Balance_kmedian, output)\n",
    "output.close()\n",
    "\n",
    "output = open('results/census/Entropy_full.pkl', 'wb')\n",
    "pickle.dump(Entropy_full, output)\n",
    "output.close()\n",
    "output = open('results/census/Entropy_mcf.pkl', 'wb')\n",
    "pickle.dump(Entropy_mcf, output)\n",
    "output.close()\n",
    "output = open('results/census/Entropy_kmedian.pkl', 'wb')\n",
    "pickle.dump(Entropy_kmedian, output)\n",
    "output.close()\n",
    "\n",
    "output = open('results/census/Like_mcf.pkl', 'wb')\n",
    "pickle.dump(Like_mcf, output)\n",
    "output.close()\n",
    "output = open('results/census/Like_kmedian.pkl', 'wb')\n",
    "pickle.dump(Like_kmedian, output)\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
